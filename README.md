ML Engineer | AI Engineer

## Contact
- Telegram: [@Int9ns9](https://t.me/Int9ns9)
- GitHub: [1NT9NS9](https://github.com/1NT9NS9)

---

## Professional Summary
Machine Learning engineer with end-to-end expertise: from data collection and preprocessing through model training to deployment and monitoring. Focused on improving models via **Supervised Fine-Tuning (SFT)**, **Reinforcement Learning with Direct Preference Optimisation (DPO)** and **LoRA fine-tuning**.

---

## Professional Activities
- Built a demand-forecasting system for construction materials using **CatBoost**.
- Developed asset-price prediction pipelines for stock and crypto markets with **XGBoost** and **LSTM**
see projects on [GitHub](https://github.com/1NT9NS9/1NT9NS9/blob/main/README(finance).md).
- Founded the Telegram channel [ROADPROFIT](https://t.me/ROADPROFIT).
- Designing an investment assistant powered by the **Mistral** LLM.

---

## Technical Skills

### Programming Languages
- **Python** (primary)

### Data Analysis & Visualisation
- **Pandas**, **NumPy** — data processing & analysis
- **matplotlib**, **Seaborn** — exploratory & executive visualisations

### Machine-Learning Frameworks & Algorithms
- **Scikit-learn** — classical algorithms
- **XGBoost**, **LightGBM**, **CatBoost** — gradient boosting

### Deep Learning & NLP
- **PyTorch** — neural-network development
- **LSTM**, **BERT** — sequence modelling
- **Hugging Face Transformers** — state-of-the-art NLP pipelines

### LLM Development
- **GPT-4**, **Gemini-2.5 Pro**, **Claude-4**, **Mistral** — model integration & evaluation
- **Supervised Fine-Tuning (SFT)** and **Reinforcement Learning via Direct Preference Optimisation (DPO)**
- **LoRA Fine-tuning** for parameter-efficient adaptation

### Tooling & DevOps
- **Docker**, **PyCharm**, **Cursor**
